---
title: "Crime Rate"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Data

```{r library, message=FALSE}
rm(list = ls())

library(tidyverse)
library(DAAG)
library(car)

crime_data = read.table("uscrime.txt.", 
                       sep="", 
                       fill=FALSE, 
                       strip.white=TRUE,
                       header = TRUE)

#test data 
crime_test <- data.frame(M = 14.0, So = 0, 
                         Ed = 10.0, Po1 = 12.0, 
                         Po2 = 15.5, LF = 0.640, 
                         M.F = 94.0, Pop = 150, 
                         NW = 1.1, U1 = 0.120,
                         U2 = 3.6, Wealth = 3200, 
                         Ineq = 20.1, Prob = 0.04, 
                         Time = 39.0)
```

Loading in the three packages that will be used throughout the problem. Next is just setting the working directory and reading in the crime data that was give to us. The crime test data is the information about the city which we are trying to predict the crime rate for. Once we build the model we are trying to predict the crime rate given those values about the city and then see how well our model does.


## Data Exploration
```{r Summary}
head(crime_data)
summary(crime_data)
summary(crime_data[,12:16])

hist(crime_data$Crime)

qqnorm(crime_data$Crime)
pairs(crime_data[,12:16])
cor(crime_data[,12:16])
#wealth and ineq are correlated
cor(crime_data)
#Po1 and Po2 highly correlated
#wealth seems to be correlated to most predictors
```

Next, lets look at the data and see what we are working with. From the head tab there seems to be 16 variables in the data set with only 47 observations. Already we know our model wont be the best because the amount of observations is really low and a lot of predictors in the set. The histogram gives us an idea of the data which should that a lot of the crime rates is beloew 1500 and even a lot of the frequency is below 1,000. The QQ plot shows that the distrubtion is fairly normal but at the end there seems to be some fluctuation. The correlation plot and chart shows how closely correlated the predicator are to each other. Wealth and Ineq seem to be highly correlated and well as Po1 and Po2. Predictors that are highly correlated could lead to false over stating the importance of a predictor.    

## LM Models
```{r}
#run some models
lm_model <- lm(Crime ~. , data = crime_data)
summary(lm_model)

plot(lm_model)

vif(lm_model)
#values greater than 10 are problematic and we have three of them
# Po1, Po2, and wealth
#data seems to be funky
dwt(lm_model)
# want to gets values close to 2 which we got 1.72
```

The first model is ran with all 15 predictors to just give us a baseline. We can see from the summary output that only 6 predictors have a significant relationship, so we are going to focus on those predictors. There are other ways of determining variable importance but for this homework we will use the previously stated method. Plotting the model gives us four different plots: first the residuals are within reason, 2nd is the qq norm which fairly normal distrubuted, 3rd the scale is never greater than 1, and finally the residuals are never greater thean the min or max. Vif and Dwt are from the car package which also tests normality. Vif values that are too large can create problems which three are and dwt you want a value lower than 2 which we achieved.      

```{r}
#cut off 0.05
lm_model2 <- lm(Crime ~ M + Ed + Ineq + Prob,
                data = crime_data)
summary(lm_model2)

#cut off 0.1
lm_model3 <- lm(Crime ~ M + Ed + Po1 + U2 + Ineq + Prob,
                data = crime_data)
summary(lm_model3)
```

I ran multiple models and decided that these two are good represenations. The first model looks at the p values less than 0.05 from the summary of the first model. The first model did output 6 varaibles but two of these did not make the .05 threshold. From the output we can see that the R-squared and adjusted R both went down signifcantly. Those R values aren't the best comparison but does show that are model is less accruate. However, the first model could be overfitting since we have so many varaibles and not a lot of data points. The third model uses a higher cut off value of 0.1 which includes the predictors Po1 and U2. The R values for the third model are a lot higher but let's see a better comparison of fit.  

```{r}
predict1 <- predict(lm_model, crime_test)
predict1

range(crime_data$Crime)

predict2 <- predict(lm_model2, crime_test)
predict2

predict3 <- predict(lm_model3, crime_test)
predict3
```

Now that we have the three models lets predict what crime rate value each of them will produce. The first model predicts a crime rate of 155 which if you remeber from the data exploration section seems a little low. Let's look at the range for crime rate  values which we see the lowest is 342. That minmum number is over double the rate that we predicted from the first model. The other two models predict a crime rate value within the current range of values. That makes me think that model 2 and 3 are going to be a better prediction than the first model.

## Best fit Model
```{r}
set.seed(4147)
#cross validate models
cv_model1 <- cv.lm(crime_data, lm_model, m=4)
#calculate the root squared error
sse <- 94720 *nrow(crime_data)
sst <- sum((crime_data$Crime - mean(crime_data$Crime))^2)
rsq <- 1 - sse / sst
rsq

cv_model2 <- cv.lm(crime_data, lm_model2, m=4)
sse2 <- 126710 * nrow(crime_data)  
sst2 <- sum((crime_data$Crime - mean(crime_data$Crime))^2)
rsq2 <- 1 - sse2 / sst2
rsq2

cv_model3 <- cv.lm(crime_data, lm_model3, m=4)
sse3 <- 48203 * nrow(crime_data)  
sst3 <- sum((crime_data$Crime - mean(crime_data$Crime))^2)
rsq3 <- 1 - sse3 / sst3
rsq3

AIC(lm_model)
AIC(lm_model2)
AIC(lm_model3)
```

Since we didn't split the data into a test or train set, we should cross validate the models in attempt to avoid overfitting. The R-squared for the first model is 0.353, the second model is 0.135, and the third model is 0.671. As you can see from the values, the third model is the best at prediciting crime rates. The second model had a lower value than the model with all the predictors which is somewhat suprising. We still could be overfitting with the first model though. The second test of fit is the AIC value which models with lower values are more accurate. Again, model 3 has the lowest value so I would say that it is the best of the models for predicting crime rate. 
